splits:
  train: 0.8
  val: 0.1
  test: 0.1

tokenize_data:
  args:
    max_length: 2048